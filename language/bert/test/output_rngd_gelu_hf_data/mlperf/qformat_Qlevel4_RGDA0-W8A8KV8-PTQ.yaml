# --weight_calib_method AMAX_SYM --weight_dtype int8 --act_calib_method PERCENTILE_ASYM --act_dtype int8 --weight_nbits 8 --act_nbits 8 --weight_granularity channel --act_granularity channel --kv_dtype int8 --kv_granularity head --is_dynamic_quant False --disable_input True --disable_output True
major_dtype:
  weight_dtype: int8
  act_dtype: int8
  weight_granularity: channel
  act_granularity: channel
  kv_dtype: int8
  kv_granularity: head
  disable_input: true
  disable_output: true
quantized op list:
  bert_embeddings_word_embeddings:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.word_embeddings._input_quantizer
        input_dtype: torch.int32
        input_shape:
        - 1
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.word_embeddings._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 30522
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
  bert_embeddings_token_type_embeddings:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.token_type_embeddings._input_quantizer
        input_dtype: torch.int32
        input_shape:
        - 1
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.token_type_embeddings._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 2
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
  bert_embeddings_position_embeddings:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.position_embeddings._input_quantizer
        input_dtype: torch.int64
        input_shape:
        - 1
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.position_embeddings._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 512
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        input_layer: true
  bert_embeddings_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.embeddings.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_0_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.0.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_1_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.1.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_2_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.2.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_3_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.3.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_4_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.4.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_5_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.5.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_6_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.6.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_7_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.7.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_8_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.8.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_9_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.9.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_10_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.10.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_11_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.11.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_12_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.12.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_13_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.13.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_14_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.14.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_15_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.15.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_16_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.16.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_17_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.17.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_18_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.18.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_19_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.19.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_20_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.20.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_21_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.21.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_22_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.22.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_attention_self_query:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.query._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.query._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_attention_self_key:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.key._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.key._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_attention_self_value:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.value._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.self.value._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_attention_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_attention_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.attention.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_intermediate_dense:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.intermediate.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.intermediate.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 1024
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_output_dense:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: int8
      axis: 2
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.output.dense._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.output.dense._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1024
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  bert_encoder_layer_23_output_layernorm:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: bert.encoder.layer.23.output.layernorm._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  qa_outputs:
    output_shape:
    - 1
    - 384
    - 2
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: qa_outputs._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        output_layer: true
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: qa_outputs._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 2
        - 1024
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
        output_layer: true
  sub:
    output_shape:
    - 1
    - 1
    - 384
    - 384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: sub._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: sub._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul:
    output_shape:
    - 1
    - 1
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_1:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_5:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_1:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_7:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_1:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_1._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_1:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_2:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_8:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_8._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_3:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_9:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_2:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_2:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_13:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_1:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_3:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_15:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_4:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_3:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_1:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_5:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_5._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_16:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_16._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_6:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_17:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_4:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_4:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_21:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_2:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_5:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_23:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_7:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_5:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_2:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_8:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_24:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_24._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_9:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_25:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_6:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_6:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_29:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_3:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_7:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_31:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_10:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_10._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_7:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_3:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_11:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_11._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_32:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_32._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_12:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_33:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_8:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_8:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_37:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_4:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_9:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_39:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_13:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_9:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_4:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_14:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_40:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_15:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_41:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_10:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_10:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_45:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_5:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_11:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_47:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_16:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_16._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_11:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_5:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_17:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_48:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_18:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_49:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_12:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_12:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_53:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_53._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_6:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_13:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_55:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_19:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_13:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_6:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_20:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_20._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_56:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_21:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_57:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_57._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_57._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_14:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_14:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_61:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_61._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_61._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_7:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_15:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_63:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_22:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_15:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_7:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_23:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_64:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_64._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_64._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_24:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_65:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_65._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_65._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_16:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_16:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_69:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_69._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_69._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_8:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_17:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_71:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_71._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_71._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_25:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_25._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_17:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_8:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_26:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_26._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_72:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_72._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_72._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_27:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_73:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_73._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_73._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_18:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_18:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_77:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_77._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_77._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_9:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_19:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_79:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_79._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_79._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_28:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_19:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_9:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_29:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_80:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_80._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_80._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_30:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_81:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_81._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_81._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_20:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_20:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_85:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_85._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_85._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_10:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_21:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_87:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_87._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_87._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_31:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_31._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_21:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_10:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_32:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_88:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_88._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_88._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_33:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_89:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_89._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_89._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_22:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_22:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_93:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_93._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_93._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_11:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_23:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_95:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_95._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_95._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_34:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_23:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_11:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_35:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_35._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_96:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_36:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_97:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_97._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_97._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_24:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_24:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_101:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_101._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_101._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_12:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_25:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_103:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_37:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_25:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_12:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_38:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_104:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_39:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_105:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_105._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_105._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_26:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_26:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_109:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_109._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_109._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_13:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_27:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_111:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_40:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_40._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_27:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_13:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_41:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_41._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_112:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_42:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_113:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_113._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_113._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_28:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_28:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_117:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_117._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_117._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_14:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_29:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_119:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_43:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_29:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_14:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_44:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_120:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_120._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_120._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_45:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_121:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_121._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_121._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_30:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_30:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_125:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_125._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_125._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_15:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_31:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_127:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_127._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_127._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_46:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_46._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_31:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_15:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_47:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_128:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_128._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_128._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_48:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_129:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_129._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_129._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_32:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_32:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_133:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_133._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_133._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_16:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_33:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_135:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_135._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_135._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_49:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_33:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_16:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_50:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_50._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_136:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_136._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_136._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_51:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_137:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_137._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_137._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_34:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_34:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_141:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_141._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_141._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_17:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_35:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_143:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_143._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_143._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_52:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_35:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_17:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_53:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_144:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_144._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_144._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_54:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_145:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_145._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_145._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_36:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_36:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_149:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_149._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_149._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_18:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_37:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_151:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_151._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_151._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_55:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_55._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_37:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_18:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_56:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_56._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_56._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_152:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_57:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_153:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_153._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_153._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_38:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_38:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_157:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_157._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_157._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_19:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_39:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_159:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_58:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_39:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_19:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_59:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_160:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_60:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_60._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_60._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_161:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_161._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_161._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_40:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_40:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_165:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_165._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_165._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_20:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_41:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_167:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_61:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_61._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_61._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_41:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_20:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_62:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_168:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_63:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_169:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_169._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_169._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_42:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_42:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_173:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_173._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_173._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_21:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_43:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_175:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_64:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_43:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_21:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_65:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_65._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_65._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_176:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_176._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_176._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_66:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_66._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_66._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_177:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_177._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_177._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_44:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_44:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_181:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_181._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_181._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_22:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_45:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_183:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_183._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_183._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_67:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_45:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_22:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_68:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_184:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_184._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_184._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_69:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_185:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_185._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_185._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_46:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 64
        - 384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_46:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  add_189:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_189._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_189._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 1
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  softmax_23:
    output_shape:
    - 1
    - 16
    - 384
    - 384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  matmul_47:
    output_shape:
    - 1
    - 16
    - 384
    - 64
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 384
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 384
        - 64
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: histogram
        calibrator_method: percentile
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  add_191:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_191._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_191._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_70:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_70._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_70._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_47:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_23:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_71:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_71._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_71._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_192:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_192._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_192._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  mul_72:
    output_shape:
    - 1
    - 384
    - 4096
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  add_193:
    output_shape:
    - 1
    - 384
    - 1024
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_193._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_193._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 1024
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  output_0_quantize_node:
    output_shape:
    - 1
    - 384
    - 2
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_0_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 384
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
